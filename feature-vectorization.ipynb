{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIN 373 UT Austin :: Jessy Li\n",
    "\n",
    "## Vectorizing categorical features\n",
    "\n",
    "### The inner workings\n",
    "\n",
    "Let's encode the Naive Bayes example we used in class into the count table shown on the slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here's the data. Let's pretend these are grammatical sentences.\n",
    "docs_train = [\"Chinese Beijing Chinese\",\n",
    "              \"Chinese Chinese Shanghai\",\n",
    "              \"Chinese Macao\",\n",
    "             \"Tokyo Japan Chinese\"]\n",
    "Y_train = [1, 1, 1, 0]\n",
    "\n",
    "docs_test = [\"Chinese Chinese Chinese Tokyo Japan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Beijing', 'Chinese'], ['Chinese', 'Chinese', 'Shanghai'], ['Chinese', 'Macao'], ['Tokyo', 'Japan', 'Chinese']]\n",
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan']]\n"
     ]
    }
   ],
   "source": [
    "## first need to tokenize each document\n",
    "docs_train_tokenized = [doc.split() for doc in docs_train]\n",
    "print(docs_train_tokenized)\n",
    "\n",
    "docs_test_tokenized = [doc.split() for doc in docs_test]\n",
    "print(docs_test_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we put words into a table?\n",
    "First, we need to create that table. The rows are just the examples. But we need to come up with the columns.\n",
    "We need to assign each word to a column number!\n",
    "We do that by creating a dictionary to map from word to a unique column id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Chinese': 0, 'Beijing': 1, 'Shanghai': 2, 'Macao': 3, 'Tokyo': 4, 'Japan': 5}\n"
     ]
    }
   ],
   "source": [
    "word_to_col_id = {}\n",
    "\n",
    "for doc in docs_train_tokenized:\n",
    "    for word in doc:\n",
    "        if word not in word_to_col_id:\n",
    "            word_to_col_id[word] = len(word_to_col_id)\n",
    "            \n",
    "print(word_to_col_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make the table, and fill it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  1.  0.  0.  0.  0.]\n",
      " [ 2.  0.  1.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.zeros((len(docs_train), len(word_to_col_id)))\n",
    "for index, doc in enumerate(docs_train_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_train[index][col_id] += 1\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same for testing docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "X_test = np.zeros((len(docs_test), len(word_to_col_id)))\n",
    "for index, doc in enumerate(docs_test_tokenized):\n",
    "    for word in doc:\n",
    "        col_id = word_to_col_id[word]\n",
    "        X_test[index][col_id] += 1\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if there is a new word in the testing docs?\n",
    "\n",
    "\n",
    "### Using a tool\n",
    "\n",
    "All of this is implemented by sklearn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), which does tokenization AND the corpus-to-table transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorizer = vectorizer.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beijing', 'chinese', 'japan', 'macao', 'shanghai', 'tokyo']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 0 0 0 0]\n",
      " [0 2 0 0 1 0]\n",
      " [0 1 0 1 0 0]\n",
      " [0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectorizer.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t2\n",
      "  (1, 4)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "## vectorizer uses sparse encoding\n",
    "print(X_train_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now to actual sentences (after NLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = [\"This new tool doesn't work.\",\n",
    "              \"We saw A Star Is Born and it was awesome.\",\n",
    "              \"The new case looks great on my phone.\",\n",
    "             \"I need a robot vacuum to rescue me from this hell.\"]\n",
    "Y_train = [0, 1, 1, 0]\n",
    "\n",
    "docs_test = [\"Have been debugging for 24 hours staight and no bugs found, awesome.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorizer = vectorizer.fit_transform(docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
